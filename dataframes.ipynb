{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d005759",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "It is an immutable distributed collection of data that is organized into\n",
    "named columns analogous to a table in a relational database. A Spark DataFrame is a similar\n",
    "concept like Python Pandas DataFrame or R Dataframe, in that it allows users to easily work with structured data.\n",
    "\n",
    "The various Spark contexts: HiveContext, SQLContext, StreamingContext, and SparkContext\n",
    "have merged together in SparkSession.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd2e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark as psc\n",
    "import pyspark.sql.context as sqlContext\n",
    "from   pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb52462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entry point into all functionality in Spark is the SparkSession class. \n",
    "# To create a basic SparkSession, just use SparkSession.builder:\n",
    "#\n",
    "#sc = SparkContext()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8082bf2",
   "metadata": {},
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "Instead of accessing the file system, let's create a DataFrame by generating the data. In this case, we'll first create the stringRDD RDD and then convert it into a DataFrame when we're reading stringJSONRDD using spark.read.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7722bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our own JSON data \n",
    "#   This way we don't have to access the file system yet.\n",
    "#  The code creates an RDD comprised of swimmers \n",
    "# (their ID, name, age, and eye color) in JSON format.\n",
    "\n",
    "stringJSONRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{sc = SparkContext()\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced352bd",
   "metadata": {},
   "source": [
    "It is important to note that parallelize , map , and mapPartitions are all RDD\n",
    "transformations. Wrapped within the DataFrame operation, spark.read.json (in\n",
    "this case), are not only the RDD transformations, but also the action which converts\n",
    "the RDD into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809601b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa72bd0",
   "metadata": {},
   "source": [
    "Note that creating the temporary table is a DataFrame transformation and not\n",
    "executed until a DataFrame action is executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef15a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary table\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eff643",
   "metadata": {},
   "source": [
    "Running the .show() method will default to present the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61201643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------+----+-------+\n",
      "|     _corrupt_record| age|eyeColor|  id|   name|\n",
      "+--------------------+----+--------+----+-------+\n",
      "|                null|  19|   brown| 123|  Katie|\n",
      "|                null|  22|   green| 234|Michael|\n",
      "|{sc = SparkContex...|null|    null|null|   null|\n",
      "+--------------------+----+--------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956384e1",
   "metadata": {},
   "source": [
    "### DataFrame queries\n",
    "\n",
    "Now that you have created the swimmersJSON DataFrame, we will be able to run\n",
    "the DataFrame API, as well as SQL queries against it. Let's start with a simple query\n",
    "showing all the rows within the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0428570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_corrupt_record=None, age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(_corrupt_record=None, age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(_corrupt_record='{sc = SparkContext()\\n    \"id\": \"345\",\\n    \"name\": \"Simone\",\\n    \"age\": 23,\\n    \"eyeColor\": \"blue\"\\n  }', age=None, eyeColor=None, id=None, name=None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Query\n",
    "# the .collect() method, which returns all the records as a list of Row objects\n",
    "# you can use either the collect() or show() method for both DataFrames and SQL queries.\n",
    "# Just make sure that if you use .collect() , this is for a small DataFrame, \n",
    "# since it will return all of the rows in the DataFrame and move them back from the \n",
    "# executors to the driver.\n",
    "# You can instead use take(<n>) or show(<n>) ,\n",
    "# which allow you to limit the number of rows returned by specifying <n>\n",
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b284cb",
   "metadata": {},
   "source": [
    "### Inferring the Schema Using Reflection\n",
    "\n",
    "Note that Apache Spark is inferring the schema using reflection; i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d0ea754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0611aa",
   "metadata": {},
   "source": [
    "Notice that Spark was able to determine infer the schema (when reviewing the schema using .printSchema).\n",
    "\n",
    "But what if we want to programmatically specify the schema?\n",
    "\n",
    "### Programmatically Specifying the Schema\n",
    "\n",
    "In this case, let's specify the schema for a CSV text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e88818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Generate our own CSV data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
    "\n",
    "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
    "schemaString = \"id name age eyeColor\"\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),    \n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Apply the schema to the RDD and Create DataFrame\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbeea945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "#   Notice that we have redefined id as Long (instead of String)\n",
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05af9f0",
   "metadata": {},
   "source": [
    "As you can see from above, we can programmatically apply the schema instead of allowing the Spark engine to infer the schema via reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315c9b8",
   "metadata": {},
   "source": [
    "### Querying with SQL\n",
    "\n",
    "With DataFrames, you can start writing your queries using Spark SQL - a SQL dialect that is compatible with the Hive Query Language (or HiveQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55914805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute SQL Query and return the data\n",
    "\n",
    "spark.sql(\"select * from swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3837911",
   "metadata": {},
   "source": [
    "Let's get the row count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f3bae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get count of rows in SQL\n",
    "spark.sql(\"select count(1) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b895d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query id and age for swimmers with age = 22 via DataFrame API\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d26e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query id and age for swimmers with age = 22 via DataFrame API in another way\n",
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e11bbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query id and age for swimmers with age = 22 in SQL\n",
    "spark.sql(\"select id, age from swimmers where age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f363f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
    "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfaadf",
   "metadata": {},
   "source": [
    "### Querying with the DataFrame API\n",
    "\n",
    "With DataFrames, you can start writing your queries using the DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a51b9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the values \n",
    "swimmers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13cbe851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, age: bigint, eyeColor: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Databricks `display` command to view the data easier\n",
    "display(swimmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "443edc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get count of rows\n",
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10f04f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the id, age where age = 22\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33c94580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the name, eyeColor where eyeColor like 'b%'\n",
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
